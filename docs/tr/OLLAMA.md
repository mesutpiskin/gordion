# Ollama Entegrasyonu

Bu dÃ¶kÃ¼man, Stash Agent'ta **yerel AI** kullanÄ±mÄ± iÃ§in **Ollama** entegrasyonunu aÃ§Ä±klar.

## ðŸ“– Ä°Ã§indekiler

1. [Ollama Nedir?](#ollama-nedir)
2. [Neden Ollama?](#neden-ollama)
3. [Kurulum](#kurulum)
4. [Model SeÃ§imi](#model-seÃ§imi)
5. [KullanÄ±m](#kullanÄ±m)
6. [OpenAI vs Ollama](#openai-vs-ollama)
7. [Troubleshooting](#troubleshooting)

---

## ðŸ¤– Ollama Nedir?

**Ollama**, aÃ§Ä±k kaynak AI modellerini (Llama, Mistral, CodeLlama vb.) **yerel bilgisayarÄ±nÄ±zda** Ã§alÄ±ÅŸtÄ±rmanÄ±zÄ± saÄŸlayan bir araÃ§tÄ±r.

### Avantajlar:
- âœ… **Ãœcretsiz** - API Ã¼cretleri yok
- âœ… **Gizlilik** - Kodunuz dÄ±ÅŸarÄ± Ã§Ä±kmaz
- âœ… **HÄ±z** - M4 Pro Max ile Ã§ok hÄ±zlÄ±
- âœ… **Offline Ã§alÄ±ÅŸma** - Ä°nternet baÄŸlantÄ±sÄ± gerektirmez
- âœ… **SÄ±nÄ±rsÄ±z kullanÄ±m** - Rate limit yok

### Dezavantajlar:
- âŒ RAM kullanÄ±mÄ± (4-40GB arasÄ±)
- âŒ Ä°lk indirme bÃ¼yÃ¼k (4-40GB arasÄ±)
- âŒ OpenAI GPT-4 kadar gÃ¼Ã§lÃ¼ deÄŸil (daha kÃ¼Ã§Ã¼k modellerde)

---

## ðŸŽ¯ Neden Ollama?

### YourCompany KullanÄ±mÄ± Ä°Ã§in Ä°deal

1. **Maliyet Tasarrufu**
   - OpenAI API Ã¼creti: ~$0.03 / 1K token
   - Ollama: **Ãœcretsiz** (sadece elektrik)
   - YÃ¼zlerce PR iÃ§in: **$$$$ tasarruf**

2. **GÃ¼venlik & Compliance**
   - Kodunuz YourCompany dÄ±ÅŸÄ±na Ã§Ä±kmaz
   - GDPR/ISO 27001 uyumlu
   - On-premise deployment

3. **Performans**
   - M4 Pro Max ile **8B model**: ~50 token/s
   - M4 Pro Max ile **70B model**: ~10-20 token/s
   - PR analizi: 10-30 saniye

---

## ðŸš€ Kurulum

### Otomatik Kurulum (Ã–nerilen)

```bash
./scripts/setup_ollama.sh
```

Bu script:
1. âœ… Ollama'yÄ± yÃ¼kler (Homebrew ile)
2. âœ… Servisi baÅŸlatÄ±r
3. âœ… Model seÃ§imi sunar
4. âœ… Modeli indirir
5. âœ… `.env` dosyasÄ±nÄ± gÃ¼nceller

### Manuel Kurulum

#### 1. Ollama YÃ¼kleme

```bash
# Homebrew ile
brew install ollama

# Veya manuel
curl -fsSL https://ollama.com/install.sh | sh
```

#### 2. Servisi BaÅŸlatma

```bash
# Arka planda baÅŸlat
ollama serve &

# Veya Ã¶n planda (log gÃ¶rmek iÃ§in)
ollama serve
```

#### 3. Model Ä°ndirme

```bash
# Ã–nerilen: Llama 3.1 8B (hÄ±zlÄ±, dengeli)
ollama pull llama3.1:8b

# Alternatif: CodeLlama 13B (kod odaklÄ±)
ollama pull codellama:13b

# Alternatif: Mistral 7B (en hÄ±zlÄ±)
ollama pull mistral:7b
```

#### 4. .env YapÄ±landÄ±rmasÄ±

```bash
AI_PROVIDER=ollama
OLLAMA_URL=http://localhost:11434
OLLAMA_MODEL=llama3.1:8b
```

---

## ðŸŽ¯ Model SeÃ§imi

### M4 Pro Max Ä°Ã§in Ã–nerilen Modeller

| Model | Boyut | RAM | HÄ±z | Kalite | KullanÄ±m |
|-------|-------|-----|-----|--------|----------|
| **llama3.1:8b** | 4.7GB | 8GB | âš¡âš¡âš¡ | â­â­â­ | **Ã–nerilen** - Genel amaÃ§lÄ± |
| **mistral:7b** | 4.1GB | 8GB | âš¡âš¡âš¡âš¡ | â­â­ | En hÄ±zlÄ±, kompakt |
| **codellama:7b** | 3.8GB | 8GB | âš¡âš¡âš¡ | â­â­â­ | Kod iÃ§in optimize |
| **llama3.1:13b** | 7.4GB | 16GB | âš¡âš¡ | â­â­â­â­ | Daha iyi kalite |
| **codellama:13b** | 7.3GB | 16GB | âš¡âš¡ | â­â­â­â­ | Kod iÃ§in en iyi |
| **llama3.1:70b** | 40GB | 64GB | âš¡ | â­â­â­â­â­ | En gÃ¼Ã§lÃ¼ (yavaÅŸ) |

### Ã–neriler

#### Genel KullanÄ±m (Ã–nerilen)
```bash
ollama pull llama3.1:8b
```
- HÄ±zlÄ± ve dengeli
- PR analizinde iyi performans
- 8GB RAM yeterli

#### Kod OdaklÄ±
```bash
ollama pull codellama:13b
```
- Kod review'da daha iyi
- Syntax ve best practice'lere odaklÄ±
- 16GB RAM gerekli

#### En HÄ±zlÄ±
```bash
ollama pull mistral:7b
```
- Ã‡ok hÄ±zlÄ± response
- Basit PR'lar iÃ§in yeterli
- 8GB RAM yeterli

#### En GÃ¼Ã§lÃ¼ (M4 Pro Max Max iÃ§in)
```bash
ollama pull llama3.1:70b
```
- GPT-4 seviyesinde
- Kompleks PR'lar iÃ§in
- 64GB RAM Ã¶nerilir
- YavaÅŸ (10-30 token/s)

---

## ðŸŽ® KullanÄ±m

### 1. Test

```bash
# Ollama baÄŸlantÄ±sÄ±nÄ± test et
python3 tests/test_ollama.py
```

Ã‡Ä±ktÄ±:
```
âœ… Ollama server is running
âœ… Model 'llama3.1:8b' is available
â³ Analyzing with Ollama AI...
âœ… Analysis Successful!

Approve: True
Confidence: 85%

Reasoning:
  The PR fixes a critical authentication bug with proper test coverage.
  Code changes are minimal and focused. No security concerns detected.
```

### 2. Dry-Run

```bash
# .env dosyasÄ±nda
AI_PROVIDER=ollama
DRY_RUN=true
RUN_MODE=once

# Ã‡alÄ±ÅŸtÄ±r
python3 src/main.py
```

### 3. Production

```bash
# .env dosyasÄ±nda
AI_PROVIDER=ollama
DRY_RUN=false
RUN_MODE=continuous

# BaÅŸlat
./agent.sh start

# LoglarÄ± izle
./agent.sh logs
```

---

## âš–ï¸ OpenAI vs Ollama

### KarÅŸÄ±laÅŸtÄ±rma Tablosu

| Ã–zellik | OpenAI (GPT-4) | Ollama (Llama 3.1 8B) | Ollama (Llama 3.1 70B) |
|---------|----------------|----------------------|------------------------|
| **Maliyet** | ~$0.03/1K token | Ãœcretsiz | Ãœcretsiz |
| **Gizlilik** | âŒ DÄ±ÅŸarÄ± gÃ¶nderir | âœ… Yerel | âœ… Yerel |
| **HÄ±z** | 1-5 saniye | 10-20 saniye | 30-60 saniye |
| **Kalite** | â­â­â­â­â­ | â­â­â­ | â­â­â­â­â­ |
| **RAM** | - | 8GB | 64GB |
| **Offline** | âŒ Ä°nternet gerekli | âœ… Offline Ã§alÄ±ÅŸÄ±r | âœ… Offline Ã§alÄ±ÅŸÄ±r |
| **Rate Limit** | âœ… Var (10K RPM) | âœ… Yok | âœ… Yok |

### Ne Zaman OpenAI?

- âœ… En iyi kalite gerekiyorsa
- âœ… HÄ±z kritikse (1-5 saniye)
- âœ… RAM sÄ±nÄ±rlÄ±ysa
- âœ… Setup yapmak istemiyorsanÄ±z

### Ne Zaman Ollama?

- âœ… Maliyet Ã¶nemliyse (Ã¼cretsiz)
- âœ… Gizlilik kritikse (on-premise)
- âœ… SÄ±nÄ±rsÄ±z kullanÄ±m gerekiyorsa
- âœ… Offline Ã§alÄ±ÅŸma istiyorsanÄ±z
- âœ… M4 Pro Max gibi gÃ¼Ã§lÃ¼ Mac'iniz varsa

---

## ðŸ”§ Troubleshooting

### Problem: Ollama'ya baÄŸlanamÄ±yor

```
âš ï¸  Cannot connect to Ollama server
```

**Ã‡Ã¶zÃ¼m:**
```bash
# Servis Ã§alÄ±ÅŸÄ±yor mu kontrol et
curl http://localhost:11434/api/tags

# EÄŸer Ã§alÄ±ÅŸmÄ±yorsa baÅŸlat
ollama serve &

# Veya Ã¶n planda
ollama serve
```

---

### Problem: Model bulunamÄ±yor

```
âš ï¸  Model 'llama3.1:8b' not available
```

**Ã‡Ã¶zÃ¼m:**
```bash
# Modeli indir
ollama pull llama3.1:8b

# Ä°ndirilen modelleri listele
ollama list
```

---

### Problem: Ã‡ok yavaÅŸ

```
â³ Analysis taking more than 60 seconds...
```

**Ã‡Ã¶zÃ¼mler:**

1. **Daha kÃ¼Ã§Ã¼k model kullan:**
   ```bash
   ollama pull mistral:7b
   # .env: OLLAMA_MODEL=mistral:7b
   ```

2. **Metal acceleration kontrol et:**
   ```bash
   # M4 GPU kullanÄ±ldÄ±ÄŸÄ±nÄ± doÄŸrula
   ollama run llama3.1:8b "test"
   # Log'da "using metal" gÃ¶rmeli
   ```

3. **RAM yeterli mi kontrol et:**
   ```bash
   # Activity Monitor'de RAM kullanÄ±mÄ±na bak
   # 8B model: 8GB
   # 13B model: 16GB
   # 70B model: 64GB
   ```

---

### Problem: JSON parse hatasÄ±

```
âš ï¸  Ollama JSON parse hatasÄ±
```

**Neden:** Model bazen JSON dÄ±ÅŸÄ±nda text ekliyor.

**Ã‡Ã¶zÃ¼m:** Kod zaten otomatik JSON extraction yapÄ±yor. EÄŸer problem devam ediyorsa:

1. Model deÄŸiÅŸtir (Llama daha tutarlÄ±)
2. Temperature azalt (0.1 - 0.3 arasÄ±)
3. Prompt'u geliÅŸtir

---

### Problem: Out of Memory

```
Error: failed to load model: out of memory
```

**Ã‡Ã¶zÃ¼m:**
```bash
# Daha kÃ¼Ã§Ã¼k model kullan
ollama pull mistral:7b

# Veya quantized model
ollama pull llama3.1:8b-q4_0  # 4-bit quantized
```

---

### Problem: Agent AI hatasÄ± diyor ama fallback mode Ã§alÄ±ÅŸÄ±yor

```
âš ï¸  AI analizi baÅŸarÄ±sÄ±z - fallback mode aktif
âœ… Auto-approved (AI failure fallback)
```

**Bu normal!** Agent, AI baÅŸarÄ±sÄ±z olsa bile PR'larÄ± approve eder (fallback mode).

**EÄŸer AI'yi dÃ¼zeltmek isterseniz:**

1. Test scripti Ã§alÄ±ÅŸtÄ±r: `python3 tests/test_ollama.py`
2. Log'larÄ± incele: `tail -f logs/agent.log`
3. Model deÄŸiÅŸtir veya Ollama'yÄ± restart et

---

## ðŸŽ“ Ä°leri Seviye

### Custom Model Kullanma

```bash
# Kendi modelinizi Modelfile ile oluÅŸturun
cat > Modelfile << 'EOF'
FROM llama3.1:8b

PARAMETER temperature 0.2
PARAMETER num_ctx 4096

SYSTEM """Sen bir YourCompany kod review uzmanÄ±sÄ±n..."""
EOF

ollama create yourcompany-reviewer -f Modelfile

# .env'de kullan
OLLAMA_MODEL=yourcompany-reviewer
```

### GPU Monitoring

```bash
# Metal performansÄ±nÄ± izle
sudo powermetrics --samplers gpu_power -i 1000

# Ollama GPU kullanÄ±mÄ±nÄ± gÃ¶r
ollama ps
```

### Model Quantization

Daha az RAM iÃ§in quantized modeller:

```bash
# 4-bit quantized (2x daha az RAM)
ollama pull llama3.1:8b-q4_0

# 8-bit quantized (1.5x daha az RAM)
ollama pull llama3.1:8b-q8_0
```

---

## ðŸ“Š Performans Metrikleri (M4 Pro Max)

### Llama 3.1 8B
- **Load time:** 2-3 saniye
- **Token/s:** 45-55 token/s
- **PR analizi:** 10-15 saniye
- **RAM:** ~8GB
- **Kalite:** GPT-3.5 seviyesi

### Llama 3.1 70B
- **Load time:** 10-15 saniye
- **Token/s:** 8-12 token/s
- **PR analizi:** 40-60 saniye
- **RAM:** ~50GB
- **Kalite:** GPT-4 seviyesi

### CodeLlama 13B
- **Load time:** 4-5 saniye
- **Token/s:** 30-40 token/s
- **PR analizi:** 15-20 saniye
- **RAM:** ~12GB
- **Kalite:** Kod iÃ§in optimize

---

## ðŸ”— FaydalÄ± Linkler

- [Ollama Official Site](https://ollama.com/)
- [Ollama Models](https://ollama.com/library)
- [Llama 3.1 Model Card](https://ollama.com/library/llama3.1)
- [CodeLlama Model Card](https://ollama.com/library/codellama)
- [Ollama API Docs](https://github.com/ollama/ollama/blob/main/docs/api.md)

---

## ðŸ’¡ Best Practices

1. **Ä°lk KullanÄ±mda Test Edin**
   ```bash
   ./scripts/setup_ollama.sh
   python3 tests/test_ollama.py
   ```

2. **KÃ¼Ã§Ã¼k BaÅŸlayÄ±n**
   - Ä°lk `llama3.1:8b` ile baÅŸlayÄ±n
   - Performans yeterliyse bu modeli kullanmaya devam edin
   - Kalite gerekiyorsa `codellama:13b` veya `llama3.1:70b` deneyin

3. **Fallback Mode'u Aktif Tutun**
   ```yaml
   auto_approve_on_ai_failure: true
   ```
   AI baÅŸarÄ±sÄ±z olsa bile agent Ã§alÄ±ÅŸmaya devam eder

4. **Monitoring YapÄ±n**
   ```bash
   ./agent.sh logs
   tail -f logs/agent.log | grep -i ollama
   ```

5. **Periyodik Restart**
   ```bash
   # Haftada bir Ollama'yÄ± restart edin (memory leak Ã¶nleme)
   pkill ollama
   ollama serve &
   ```

---

**ðŸŽ‰ ArtÄ±k YourCompany Stash Agent, tamamen yerel AI ile Ã§alÄ±ÅŸÄ±yor!**
